{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepagents tavily-python\n",
    "!pip install tavily\n",
    "!pip install deepagents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f2d8b",
   "metadata": {},
   "source": [
    "### Define model and set api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec782ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "  os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter API key for Tavily: \")\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\",\n",
    "                        temperature = 0.7,\n",
    "                        timeout=30,\n",
    "                        max_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c4d8c",
   "metadata": {},
   "source": [
    "## Create Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9ebba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from tavily import TavilyClient\n",
    "from deepagents import create_deep_agent\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "def internet_search(\n",
    "    query: str,\n",
    "    max_results: int = 5,\n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
    "    include_raw_content: bool = False,\n",
    "):\n",
    "    \"\"\"Run a web search\"\"\"\n",
    "    return tavily_client.search(\n",
    "        query,\n",
    "        max_results=max_results,\n",
    "        include_raw_content=include_raw_content,\n",
    "        topic=topic,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875ba65",
   "metadata": {},
   "source": [
    "## Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d9902d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt to steer the agent to be an expert researcher\n",
    "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
    "You have access to an internet search tool as your primary means of gathering information.\n",
    "## `internet_search`\n",
    "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
    "\"\"\"\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    model=model,\n",
    "    tools=[internet_search],\n",
    "    system_prompt=research_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49a76c",
   "metadata": {},
   "source": [
    "### Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed2e4fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Transformer model is a type of neural network architecture that has '\n",
      " 'revolutionized natural language processing (NLP) tasks. It was introduced in '\n",
      " 'the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.\\n'\n",
      " '\\n'\n",
      " \"Here's a breakdown of its key components and concepts:\\n\"\n",
      " '\\n'\n",
      " '**1. Architecture:**\\n'\n",
      " '   - The Transformer architecture consists of two main parts: an **encoder** '\n",
      " 'and a **decoder**.\\n'\n",
      " '   - Both the encoder and decoder are typically composed of multiple '\n",
      " 'identical layers stacked on top of each other.\\n'\n",
      " '\\n'\n",
      " '**2. Core Idea: Self-Attention:**\\n'\n",
      " '   - The most significant innovation of the Transformer is the '\n",
      " '**self-attention mechanism**.\\n'\n",
      " '   - Unlike previous models (like RNNs and LSTMs) that processed sequences '\n",
      " 'word-by-word, self-attention allows the model to weigh the importance of '\n",
      " 'different words in the input sequence when processing a particular word.\\n'\n",
      " '   - This means the model can capture long-range dependencies and '\n",
      " 'relationships between words, regardless of their distance in the sequence.\\n'\n",
      " '\\n'\n",
      " '**3. How Self-Attention Works (Simplified):**\\n'\n",
      " '   - For each word in the input sequence, the model creates three vectors: a '\n",
      " '**Query (Q)**, a **Key (K)**, and a **Value (V)**.\\n'\n",
      " '   - To determine how much attention a word should pay to other words, it '\n",
      " 'compares its Query vector with the Key vectors of all other words (including '\n",
      " 'itself).\\n'\n",
      " '   - The result of this comparison (often a dot product) is then scaled and '\n",
      " 'passed through a softmax function to get attention weights. These weights '\n",
      " 'indicate the importance of each word.\\n'\n",
      " '   - Finally, these attention weights are used to compute a weighted sum of '\n",
      " 'the Value vectors of all words. This weighted sum becomes the output '\n",
      " 'representation for the current word, incorporating context from the entire '\n",
      " 'sequence.\\n'\n",
      " '\\n'\n",
      " '**4. Positional Encoding:**\\n'\n",
      " \"   - Since self-attention processes words in parallel and doesn't inherently \"\n",
      " 'consider their order, the Transformer adds **positional encodings** to the '\n",
      " 'input embeddings.\\n'\n",
      " '   - These encodings provide information about the position of each word in '\n",
      " 'the sequence, allowing the model to understand word order.\\n'\n",
      " '\\n'\n",
      " '**5. Multi-Head Attention:**\\n'\n",
      " '   - Instead of performing self-attention just once, the Transformer uses '\n",
      " '**multi-head attention**.\\n'\n",
      " '   - This means it runs the self-attention mechanism multiple times in '\n",
      " 'parallel, each with different learned Query, Key, and Value projections.\\n'\n",
      " '   - Each \"head\" can learn to focus on different aspects of the '\n",
      " 'relationships between words, leading to a richer representation.\\n'\n",
      " '\\n'\n",
      " '**6. Encoder and Decoder Layers:**\\n'\n",
      " '   - **Encoder:** Takes the input sequence, processes it through '\n",
      " 'self-attention and feed-forward layers, and outputs a contextualized '\n",
      " 'representation.\\n'\n",
      " \"   - **Decoder:** Takes the encoder's output and the previously generated \"\n",
      " 'output sequence to produce the next element in the output sequence. It also '\n",
      " 'uses a masked self-attention mechanism to ensure that predictions for a '\n",
      " 'position only depend on known outputs at previous positions.\\n'\n",
      " '\\n'\n",
      " '**7. Applications:**\\n'\n",
      " '   - The Transformer architecture has been highly successful in various NLP '\n",
      " 'tasks, including:\\n'\n",
      " '     - Machine Translation\\n'\n",
      " '     - Text Summarization\\n'\n",
      " '     - Text Generation (e.g., GPT models)\\n'\n",
      " '     - Question Answering\\n'\n",
      " '     - Sentiment Analysis\\n'\n",
      " '\\n'\n",
      " \"**In essence, the Transformer's ability to efficiently capture long-range \"\n",
      " 'dependencies through self-attention has made it a cornerstone of modern NLP '\n",
      " 'models.**')\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Explain transformer model.\"}]})\n",
    "\n",
    "from pprint import pprint\n",
    "# Print the agent's response\n",
    "pprint(result[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
