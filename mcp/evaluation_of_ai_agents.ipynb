{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a83d444",
   "metadata": {},
   "source": [
    "#### AI Agent Evaluation\n",
    "\n",
    "Agents are typically made up of three (3) main components:\n",
    "- A Router: can be llm with function calling, rule-based code or simple classifier\n",
    "- Skills e.g. lookup item, answer question etc.\n",
    "- Memory and State: used to store - retrieved context, config variables, previous agent execution steps etc.\n",
    "\n",
    "Agent Evaluation can be divide into two(2) main steps\n",
    "\n",
    "- LLM (Model) Evaluation\n",
    "- LLM Application Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7dcab",
   "metadata": {},
   "source": [
    "#### Common Types of Evaluations for LLM Systems\n",
    "- Hallucinations\n",
    "- Retrieval relevance\n",
    "- Q & A on retrieved data\n",
    "- Toxicity\n",
    "- Summarization performance\n",
    "- Code writing correctness and readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93464348",
   "metadata": {},
   "source": [
    "#### Tools used to evaluate agents\n",
    "- Phoenix Server\n",
    "- Open Telemetry (OTEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29b82d",
   "metadata": {},
   "source": [
    "#### Techniques for Running Evaluations on AI Agent\n",
    "- Code-based Evals: regex matches, json parseable, contains keyword(s) etc.\n",
    "    - Direct match\n",
    "    - Semantic match: cosine similarity/distance\n",
    "- LLM-As-A-Judge Evals\n",
    "- Human Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db92b58",
   "metadata": {},
   "source": [
    "##### Evaluating Rounter\n",
    "- Function calling choice\n",
    "- Parameter extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce28bc",
   "metadata": {},
   "source": [
    "Prompt for Using LLM-As-A-Judge for Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3baca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tool_Calling_Prompt_Template = \"\"\"\n",
    "You are an evaluation assistant evaluating questions and tool calls to determine whether the tool called would answer the question. The tool calls have been generated by a separate agent, and chosen from the list of tools provided below. It is your task to decide whether that agent chose the right tool to call.\n",
    "\n",
    "[BEGIN DATA]\n",
    "*****************\n",
    "[Question]: {question}\n",
    "*****************\n",
    "[Tool Called]: {tool_call}\n",
    "[END DATA]\n",
    "\n",
    "Your response must be single word, either \"correct or \"incorrect\", and should not contain any text or characters aside from that word.\n",
    "\"incorrect\" means that the chosen tool would not answer the question, the tool includes information that is not presented in the question, or that the tool signature includes parameter values that don't match the formats specified in the tool signatures below.\n",
    "\"correct\" means that correct tool call was chosen, the correct parameters were extracted from the question, the tool call generated is runnable and correct, and that no outside information not present in the question was used in the generated question.\n",
    "\n",
    "[Tool Definitions]: {tool_definitions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1692acb7",
   "metadata": {},
   "source": [
    "##### Evaluating Skills\n",
    "\n",
    "- Relevance\n",
    "- Hallucination\n",
    "- Question and answer correctness\n",
    "- Generated code readability\n",
    "- Summarization\n",
    "- Regex\n",
    "- JSON parseable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830ae05",
   "metadata": {},
   "source": [
    "#### Trajectory Evaluation\n",
    "- Convergence: convergence measures how closely our agent follows the optimal path for a given query.\n",
    "\n",
    "$$ convergence = \\frac{\\Sigma^{N}_{i} min (1, \\frac{S_{optimal}}{S_{agent, i}})}{N} $$\n",
    "\n",
    "where s = steps taken by agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4254f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['C:\\\\Windows\\\\System32\\\\calc.exe'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(['C:\\\\Windows\\\\System32\\\\calc.exe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0c05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc = \"\"\"def myfunc():\n",
    "                x = -2*i\n",
    "                print(x)\n",
    "                prit(\"This is a test\")\n",
    "        \"\"\"\n",
    "\n",
    "exec(myfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf8dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
